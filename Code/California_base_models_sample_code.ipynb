{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJKGRmyTJ6iuXtkY1fBwJo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rCSiA-kK7D9j"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from datetime import timedelta, datetime\n","\n","\n","import xgboost as xgb\n","\n","# For tuning\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from time import time\n","\n","# Keras\n","from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasRegressor\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import mean_squared_error\n","\n","\n","# loocv\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","#import osgeo as gdal\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import datetime\n","import geopandas as gpd\n","import rasterio as rio\n","import shutil, os\n","import shapely\n","import csv\n","import os\n","from rasterio.plot import show\n","from rasterio.warp import calculate_default_transform, reproject, Resampling\n","\n","from numpy.random import randn\n","from numpy.random import seed\n","from scipy.stats import pearsonr\n","from matplotlib import pyplot\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import hydroeval as he"]},{"cell_type":"code","source":["ASO_files = ['CE20160401Data_set_SWE.csv', 'LB20160614_Data_set_SWE.csv', 'SJ20180304_Data_set_SWE.csv',\n","            'CE20170816Data_set_SWE.csv', 'TB20130403_Data_set_SWE.csv', 'MB20180425_Data_set_SWE.csv', 'SF20180423_Data_set_SWE.csv',\n","            'KC20190608_Data_set_SWE.csv', 'CE20160407Data_set_SWE.csv','LB20160621_Data_set_SWE.csv', 'TE20190503_Data_set_SWE.csv',\n","            'TB20130429_Data_set_SWE.csv', 'SJ20180422_Data_set_SWE.csv', 'LB20190715_Data_set_SWE.csv', 'TB20160527_Data_set_SWE.csv',\n","            'CE20160416Data_set_SWE.csv', 'LB20160626_Data_set_SWE.csv', 'KN20190417_Data_set_SWE.csv', 'SJ20190704_Data_set_SWE.csv',\n","            'TB20130525_Data_set_SWE.csv', 'TB20130503_Data_set_SWE.csv', 'LB20190703_Data_set_SWE.csv', 'SF20190502_Data_set_SWE.csv',\n","            'CE20160426Data_set_SWE.csv', 'TB20160708_Data_set_SWE.csv', 'LB20190501_Data_set_SWE.csv', 'SJ20180601_Data_set_SWE.csv',\n","            'TB20170727_Data_set_SWE.csv', 'MB20190716_Data_set_SWE.csv', 'TB20170129_Data_set_SWE.csv',\n","            'CE20160509Data_set_SWE.csv', 'SF20190609_Data_set_SWE.csv', 'TB20170717_Data_set_SWE.csv','TE20190705_Data_set_SWE.csv',\n","            'SJ20190325_Data_set_SWE.csv', 'TB20160509_Data_set_SWE.csv', 'LV20170717_Data_set_SWE.csv', 'MB20190703_Data_set_SWE.csv',\n","            'CE20170129Data_set_SWE.csv', 'KC20190427_Data_set_SWE.csv', 'TB20170816_Data_set_SWE.csv', 'LB20170718_Data_set_SWE.csv',\n","            'SF20190704_Data_set_SWE.csv', 'SJ20190713_Data_set_SWE.csv', 'LB20170128_Data_set_SWE.csv', 'TE20190417_Data_set_SWE.csv',\n","            'CE20170717Data_set_SWE.csv', 'TB20160426_Data_set_SWE.csv', 'LB20170815_Data_set_SWE.csv', 'KN20190611_Data_set_SWE.csv',\n","            'TB20160416_Data_set_SWE.csv', 'SJ20190501_Data_set_SWE.csv', 'RC20170717_Data_set_SWE.csv', 'SF20190317_Data_set_SWE.csv',\n","            'CE20170727Data_set_SWE.csv', 'TB20180528_Data_set_SWE.csv', 'TE20190613_Data_set_SWE.csv',\n","            'TB20160407_Data_set_SWE.csv', 'SJ20170815_Data_set_SWE.csv', 'MB20190604_Data_set_SWE.csv', 'LB20190611_Data_set_SWE.csv',\n","            'TB20160401_Data_set_SWE.csv', 'KN20180426_Data_set_SWE.csv', 'LB20190309_Data_set_SWE.csv', 'TB20180423_Data_set_SWE.csv',\n","            'SF20180601_Data_set_SWE.csv', 'TB20160326_Data_set_SWE.csv', 'SJ20190614_Data_set_SWE.csv', 'SF20190714_Data_set_SWE.csv',\n","            'CE20180423Data_set_SWE.csv', 'TB20130608_Data_set_SWE.csv', 'LB20180601_Data_set_SWE.csv', 'TE20190324_Data_set_SWE.csv',\n","            'TB20130601_Data_set_SWE.csv', 'SJ20170719_Data_set_SWE.csv', 'KC20190326_Data_set_SWE.csv', 'SF20170718_Data_set_SWE.csv',\n","            'CE20180528Data_set_SWE.csv', 'MB20190329_Data_set_SWE.csv', 'LB20180422_Data_set_S']"],"metadata":{"id":"WpKs1iLW8FMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Hyperparameter_tuning(Training_files, Test_files, Validation_files, NUM_EPOCHS, saving_file_path):\n","\n","  '''\n","  This function performs a bayesian hyperparameter optimization using the Training_files and Validation_files.\n","  It tests the models with the optimal 'best_parameters' on the Test_files.\n","  It saves the model in saving_file_path along with the a true vs predicted scatter plot and the training learning curves.\n","  It returns best_parameters, nse (R^2), and pearson correlation values on the Test_files.\n","  '''\n","\n","    dataframe_train = pandas.read_csv(Training_files[0], index_col = 0 )\n","\n","    for file in Training_files[1 :]:\n","        df_train= pandas.read_csv(file, index_col = 0 )\n","        dataframe_train = dataframe_train.append(df_train, ignore_index = True)\n","\n","    X_train = dataframe_train.values[:, 2:9]\n","    y_train = dataframe_train.values[:, 9]\n","\n","    #Validation set\n","    dataframe_val= pandas.read_csv(Validation_files[0], index_col = 0 )\n","\n","    for file in Validation_files[1 :]:\n","        df_val= pandas.read_csv(file, index_col = 0 )\n","        dataframe_val = dataframe_val.append(df_val, ignore_index = True)\n","\n","    X_val = dataframe_val.values[:, 2:9]\n","    y_val = dataframe_val.values[:, 9]\n","\n","    # Testing dataset\n","    dataframe_test= pandas.read_csv(Test_files[0], index_col = 0 )\n","\n","    for file in Test_files[1 :]:\n","        df_test= pandas.read_csv(file, index_col = 0 )\n","        dataframe_test = dataframe_test.append(df_test, ignore_index = True)\n","\n","    X_test = dataframe_test.values[:, 2:9]\n","    y_test = dataframe_test.values[:, 9]\n","\n","\n","    #3: Need to scale the features for neural networks, otherwise the training doesn't converge.\n","    scaler = MinMaxScaler()\n","    scaler.fit(X_train)\n","    X_train_scaled = scaler.transform(X_train)\n","    X_val_scaled = scaler.transform(X_val)\n","    X_test_scaled =  scaler.transform(X_test)\n","\n","    # This returns a multi-layer-perceptron model in Keras.\n","    def get_keras_model(num_hidden_layers,\n","                        num_neurons_per_layer,\n","                        dropout_rate,\n","                        activation):\n","        # create the MLP model.\n","\n","        # define the layers.\n","        inputs = tf.keras.Input(shape=(X_train_scaled.shape[1],))  # input layer.\n","        x = layers.Dropout(dropout_rate)(inputs) # dropout on the weights.\n","\n","        # Add the hidden layers.\n","        for i in range(num_hidden_layers):\n","            x = layers.Dense(num_neurons_per_layer,\n","                             activation=activation)(x)\n","            x = layers.Dropout(dropout_rate)(x)\n","\n","        # output layer.\n","        outputs = layers.Dense(1, activation='linear')(x)\n","\n","        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","        return model\n","\n","\n","    # This function takes in the hyperparameters and returns a score (Cross validation).\n","    def keras_mlp_cv_score(parameterization, weight=None):\n","\n","        model = get_keras_model(parameterization.get('num_hidden_layers'),\n","                                parameterization.get('neurons_per_layer'),\n","                                parameterization.get('dropout_rate'),\n","                                parameterization.get('activation'))\n","\n","        opt = parameterization.get('optimizer')\n","        opt = opt.lower()\n","\n","        learning_rate = parameterization.get('learning_rate')\n","\n","        if opt == 'adam':\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","        elif opt == 'rms':\n","            optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","        else:\n","            optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","\n","\n","        # Specify the training configuration.\n","        model.compile(optimizer=optimizer,\n","                      loss=tf.keras.losses.MeanSquaredError(),\n","                      metrics=['mse'])\n","\n","        data = X_train_scaled\n","        labels = y_train\n","\n","        # fit the model using a the cross validation set.\n","        res = model.fit(data, labels, epochs=NUM_EPOCHS, batch_size=parameterization.get('batch_size'), validation_data = (X_val_scaled, y_val))\n","\n","        # look at the last 10 epochs. Get the mean and standard deviation of the validation score.\n","        last10_scores = np.array(res.history['val_loss'][-10:])\n","        mean = last10_scores.mean()\n","        sem = last10_scores.std()\n","\n","        # If the model didn't converge then set a high loss.\n","        if np.isnan(mean):\n","            return 9999.0, 0.0\n","\n","        return mean, sem\n","\n","    # Define the search space.\n","    parameters=[\n","        {\n","            \"name\": \"learning_rate\",\n","            \"type\": \"range\",\n","            \"bounds\": [0.0001, 0.5],\n","            \"log_scale\": True,\n","        },\n","        {\n","            \"name\": \"dropout_rate\",\n","            \"type\": \"range\",\n","            \"bounds\": [0.0001, 0.5],\n","            \"log_scale\": True,\n","        },\n","        {\n","            \"name\": \"num_hidden_layers\",\n","            \"type\": \"range\",\n","            \"bounds\": [7, 30],\n","            \"value_type\": \"int\"\n","        },\n","        {\n","            \"name\": \"neurons_per_layer\",\n","            \"type\": \"range\",\n","            \"bounds\": [40, 300],\n","            \"value_type\": \"int\"\n","        },\n","        {\n","            \"name\": \"batch_size\",\n","            \"type\": \"choice\",\n","            \"values\": [8, 16, 32, 64, 128, 256, 512],\n","        },\n","\n","        {\n","            \"name\": \"activation\",\n","            \"type\": \"choice\",\n","            \"values\": ['tanh', 'sigmoid', 'relu'],\n","        },\n","        {\n","            \"name\": \"optimizer\",\n","            \"type\": \"choice\",\n","            \"values\": ['adam', 'rms', 'sgd'],\n","        },\n","    ]\n","\n","    # import more packages\n","    from ax.service.ax_client import AxClient\n","    from ax.utils.notebook.plotting import render, init_notebook_plotting\n","    from ax.modelbridge.generation_strategy import GenerationStrategy, GenerationStep\n","    from ax.modelbridge.registry import Models, ModelRegistryBase\n","    from ax.modelbridge.dispatch_utils import choose_generation_strategy\n","    from ax.modelbridge.modelbridge_utils import get_pending_observation_features\n","\n","    from ax.utils.testing.core_stubs import get_branin_search_space, get_branin_experiment\n","    init_notebook_plotting()\n","\n","    ax_client = AxClient()\n","\n","    # create the experiment.\n","    ax_client.create_experiment(\n","        name=\"keras_experiment\",\n","        parameters=parameters,\n","        objective_name='keras_cv',\n","        minimize=True)\n","\n","    def evaluate(parameters):\n","        return {\"keras_cv\": keras_mlp_cv_score(parameters)}\n","\n","\n","\n","    # Based on the search space, this will use Sobol (https://en.wikipedia.org/wiki/Sobol_sequence) instead of a Guassian Process (GPEI).\n","    #\n","    # From the source code:\n","    #     \"\"\"We should use only Sobol and not GPEI if:\n","    #     1. there are less continuous parameters in the search space than the sum of\n","    #     options for the choice parameters,\n","    #     2. the number of total iterations in the optimization is known in advance and\n","    #     there are less distinct points in the search space than the known intended\n","    #     number of total iterations.\n","    #     \"\"\"\n","\n","    # Sobol sequences seem to be better than random for high-dimensional spaces:\n","    # https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method\n","\n","    for i in range(35):\n","        parameters, trial_index = ax_client.get_next_trial()\n","        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters))\n","\n","    # look at all the trials.\n","    ax_client.get_trials_data_frame().sort_values('trial_index')\n","\n","    best_parameters, values = ax_client.get_best_parameters()\n","\n","    # the best set of parameters.\n","    for k in best_parameters.items():\n","        print(k)\n","\n","    print()\n","\n","    # the best score achieved.\n","    means, covariances = values\n","    print(means)\n","\n","    keras_model = get_keras_model(best_parameters['num_hidden_layers'],\n","                              best_parameters['neurons_per_layer'],\n","                              best_parameters['dropout_rate'],\n","                              best_parameters['activation'])\n","\n","    opt = best_parameters['optimizer']\n","    opt = opt.lower()\n","\n","    learning_rate = best_parameters['learning_rate']\n","\n","    if opt == 'adam':\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    elif opt == 'rms':\n","        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","    else:\n","        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","\n","    # Specify the training configuration.\n","    keras_model.compile(optimizer=optimizer,\n","                  loss=tf.keras.losses.MeanSquaredError(),\n","                  metrics=['mse'])\n","\n","    data = X_train_scaled\n","    labels = y_train\n","    res = keras_model.fit(data, labels, validation_data = (X_val_scaled, y_val), epochs=NUM_EPOCHS, batch_size=best_parameters['batch_size'])\n","\n","\n","    keras_model.save( filepath = saving_file_path )\n","\n","    # Plot learning curve\n","    fig = matplotlib.pyplot.figure()\n","    plt.plot(res.history['loss'])\n","    plt.plot(res.history['val_loss'])\n","    plt.title('model loss on basin ')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'validation'], loc='upper left')\n","    plt.show()\n","\n","    fig.savefig( saving_file_path  +'/BM_LearningCurve.pdf')\n","\n","    yhat = keras_model.predict(X_test_scaled)\n","    print(mean_squared_error(y_test, yhat))\n","    fig = matplotlib.pyplot.figure()\n","    plt.plot(y_test, label = 'observed')\n","    plt.plot(yhat, label = 'predicted')\n","    plt.xlabel('data point number')\n","    plt.ylabel('SWE (m)')\n","    plt.legend()\n","    fig.savefig(saving_file_path + '/BM_plot.pdf')\n","\n","    fig = matplotlib.pyplot.figure()\n","    pyplot.scatter(y_test, yhat, c=dataframe_test['DEM'], cmap='terrain', vmin=2500, vmax=4100)\n","    plt.xlabel('SWE observed (m)')\n","    plt.ylabel('SWE predicted (m)')\n","    fig.savefig(saving_file_path  + '/BM_scatter.pdf')\n","    pyplot.show()\n","    # calculate the Pearson's correlation between two variables\n","    # seed random number generator\n","    seed(1)\n","    corr, _ = pearsonr(y_test, yhat.flatten())\n","    print('Pearsons correlation: %.3f' % corr)\n","\n","    #Calculate NSE coefficient\n","    nse = he.evaluator(he.nse, yhat, y_test)\n","    print('NSE: %.3f' % nse)\n","\n","\n","    return nse, corr, best_parameters\n"],"metadata":{"id":"HMrw49Cy8Frf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For Base model 3 ( which is used in transfer learning)\n","Test_files = ASO_files[65: 80]\n","Training_files = ASO_files[0: 26] + ASO_files[39: 65]# 50 files\n","Validation_files = ASO_files[26: 39] # 15 files\n","saving_file_path '/home/users/lie08/California_base_models/Base_Model_3'\n","NUM_EPOCHS = 700\n","nse, corr, best_parameters= Hyperparameter_tuning(Training_files, Test_files, Validation_files, NUM_EPOCHS, saving_file_path)\n","print(nse)\n","print(corr)\n","print(best_parameters)\n","# load csv module\n","import csv\n","\n","# define a dictionary with key value pairs\n","dict = {}\n","dict['NSE'] = nse\n","dict['R'] = corr\n","dict['learning_rate'] = best_parameters['learning_rate']\n","dict['dropout_rate'] = best_parameters['dropout_rate']\n","dict['num_hidden_layers'] = best_parameters['num_hidden_layers']\n","dict['neurons_per_layer'] = best_parameters['neurons_per_layer']\n","dict['batch_size'] = best_parameters['batch_size']\n","dict['activation'] = best_parameters['activation']\n","dict['optimizer'] = best_parameters['optimizer']\n","print(dict)\n","\n","# open file for writing, \"w\" is writing\n","csvout = open(\"BM3.csv\", \"w\")\n","w = csv.writer(csvout)\n","\n","# loop over dictionary keys and values\n","for key, val in dict.items():\n","\n","    # write every key and value to file\n","    w.writerow([key, val])\n","csvout.close()\n"],"metadata":{"id":"sTBJglue_P5E"},"execution_count":null,"outputs":[]}]}